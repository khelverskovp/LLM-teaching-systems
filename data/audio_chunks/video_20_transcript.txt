 The important thing here is that we actually have to kind of store a lot of things in the memory and this is usually not a problem if you have something like let's say like here we have three layers of weights but if you think about this for what is called recurrent on network then actually kind of the depth is like the number of time steps and then this can actually be a little bit memory consuming. So we come back to that when we talk about recurrent networks.