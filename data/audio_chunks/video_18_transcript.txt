 we can simplify the computation for the gradient in the first layer and we don't have this kind of exploding complexity. So this leads us to define this backpropagation algorithm which is nothing but like a dynamic programming algorithm where we can write it like this and you can see that in general we can write recursion where we say that the derivatives of the activation in of the cost function in one layer is defined in general.