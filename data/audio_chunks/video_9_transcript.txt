 respect to each of the elements, each of the parameters and models. So if we have end parameters, then we get the gradient as an end dimensional vector. And then our update rule, our basic upgrade rule is to take a small step opposite the gradient. So the gradient, if you think about margin climbing, tells us what is kind of the steepest direction to go. And now we take a step opposite that. So we kind of go down, moves downhill as we can. And then we have a learning rate, ETA, which might depend on which step we have taken, which happens.