 terms of the derivative of the activations in the next layer. So you can see this is why it's called back propagation. So the forward propagation is when we propagate the input to the output, calculate the cost and then the back propagation is through this kind of linear recursion for the gradients. And then the end, when we have done this kind of back propagation in the middle equation, then we can actually compute the actual gradient simply by the last of the rule and the